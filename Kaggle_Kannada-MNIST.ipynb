{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "71910804-4b34-4583-9b78-1f48421506c8",
    "_uuid": "303b30dc-dcdc-4e84-a742-26a313059baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/emnist/emnist-bymerge-mapping.txt\n",
      "/kaggle/input/emnist/emnist-byclass-mapping.txt\n",
      "/kaggle/input/emnist/emnist-byclass-test.csv\n",
      "/kaggle/input/emnist/emnist-digits-train.csv\n",
      "/kaggle/input/emnist/emnist-mnist-train.csv\n",
      "/kaggle/input/emnist/emnist-letters-train.csv\n",
      "/kaggle/input/emnist/emnist-balanced-train.csv\n",
      "/kaggle/input/emnist/emnist-digits-mapping.txt\n",
      "/kaggle/input/emnist/emnist-digits-test.csv\n",
      "/kaggle/input/emnist/emnist-bymerge-train.csv\n",
      "/kaggle/input/emnist/emnist-mnist-mapping.txt\n",
      "/kaggle/input/emnist/emnist-balanced-mapping.txt\n",
      "/kaggle/input/emnist/emnist-balanced-test.csv\n",
      "/kaggle/input/emnist/emnist-mnist-test.csv\n",
      "/kaggle/input/emnist/emnist-byclass-train.csv\n",
      "/kaggle/input/emnist/emnist-letters-test.csv\n",
      "/kaggle/input/emnist/emnist-letters-mapping.txt\n",
      "/kaggle/input/emnist/emnist-bymerge-test.csv\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-digits-test-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-mnist-test-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-bymerge-test-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-digits-test-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-mnist-train-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-byclass-test-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-byclass-train-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-balanced-train-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-balanced-test-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-balanced-test-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-bymerge-train-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-letters-train-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-digits-train-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-mnist-test-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-bymerge-test-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-balanced-train-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-mnist-train-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-digits-train-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-byclass-train-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-bymerge-train-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-byclass-test-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-letters-train-labels-idx1-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-letters-test-images-idx3-ubyte\n",
      "/kaggle/input/emnist/emnist_source_files/emnist-letters-test-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/t10k-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/train-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/train-images-idx3-ubyte\n",
      "/kaggle/input/fashionmnist/fashion-mnist_test.csv\n",
      "/kaggle/input/fashionmnist/fashion-mnist_train.csv\n",
      "/kaggle/input/fashionmnist/t10k-images-idx3-ubyte\n",
      "/kaggle/input/Kannada-MNIST/test.csv\n",
      "/kaggle/input/Kannada-MNIST/train.csv\n",
      "/kaggle/input/Kannada-MNIST/Dig-MNIST.csv\n",
      "/kaggle/input/Kannada-MNIST/sample_submission.csv\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,773,642\n",
      "Trainable params: 1,771,082\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Conv2D(64, (3,3),input_shape = (28,28,1)))\n",
    "model1.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.Conv2D(64, (3,3)))\n",
    "model1.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "model1.add(tf.keras.layers.Dropout(0.30))\n",
    "\n",
    "\n",
    "model1.add(tf.keras.layers.Conv2D(128, (3,3), padding = 'same'))\n",
    "model1.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.Conv2D(128, (3,3), padding = 'same'))\n",
    "model1.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "model1.add(tf.keras.layers.Dropout(0.30))\n",
    "\n",
    "\n",
    "model1.add(tf.keras.layers.Conv2D(256, (3,3), padding = 'same'))\n",
    "model1.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.Conv2D(256, (3,3), padding = 'same'))\n",
    "model1.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=\"uniform\"))\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "model1.add(tf.keras.layers.Dropout(0.30))\n",
    "\n",
    "\n",
    "model1.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model1.add(tf.keras.layers.Dense(256))\n",
    "model1.add(tf.keras.layers.BatchNormalization())\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.Dropout(0.30))\n",
    "model1.add(tf.keras.layers.Dense(128))\n",
    "model1.add(tf.keras.layers.BatchNormalization())\n",
    "model1.add(tf.keras.layers.LeakyReLU(0.1))\n",
    "model1.add(tf.keras.layers.Dropout(0.20))\n",
    "\n",
    "model1.add(tf.keras.layers.Dense(10 , activation = tf.nn.softmax))\n",
    "\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0015), loss = \"categorical_crossentropy\" , metrics=['acc'])\n",
    "model1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 53 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "53/53 [==============================] - 30s 566ms/step - loss: 1.1643 - acc: 0.5732 - val_loss: 2.3514 - val_acc: 0.0945\n",
      "Epoch 2/20\n",
      "53/53 [==============================] - 26s 488ms/step - loss: 0.7280 - acc: 0.7275 - val_loss: 1.3387 - val_acc: 0.6906\n",
      "Epoch 3/20\n",
      "53/53 [==============================] - 26s 490ms/step - loss: 0.6195 - acc: 0.7708 - val_loss: 1.0874 - val_acc: 0.7844\n",
      "Epoch 4/20\n",
      "53/53 [==============================] - 26s 488ms/step - loss: 0.5663 - acc: 0.7922 - val_loss: 1.0721 - val_acc: 0.6352\n",
      "Epoch 5/20\n",
      "53/53 [==============================] - 26s 485ms/step - loss: 0.5290 - acc: 0.8047 - val_loss: 0.5789 - val_acc: 0.8117\n",
      "Epoch 6/20\n",
      "53/53 [==============================] - 26s 485ms/step - loss: 0.4975 - acc: 0.8174 - val_loss: 0.6709 - val_acc: 0.7437\n",
      "Epoch 7/20\n",
      "53/53 [==============================] - 26s 491ms/step - loss: 0.4777 - acc: 0.8234 - val_loss: 0.6320 - val_acc: 0.7727\n",
      "Epoch 8/20\n",
      "53/53 [==============================] - 27s 503ms/step - loss: 0.4571 - acc: 0.8319 - val_loss: 0.3696 - val_acc: 0.8766\n",
      "Epoch 9/20\n",
      "53/53 [==============================] - 26s 493ms/step - loss: 0.4434 - acc: 0.8363 - val_loss: 0.4844 - val_acc: 0.8328\n",
      "Epoch 10/20\n",
      "53/53 [==============================] - 26s 489ms/step - loss: 0.4311 - acc: 0.8413 - val_loss: 0.4252 - val_acc: 0.8359\n",
      "Epoch 11/20\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8442\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00045000000391155477.\n",
      "53/53 [==============================] - 26s 496ms/step - loss: 0.4182 - acc: 0.8440 - val_loss: 0.3790 - val_acc: 0.8555\n",
      "Epoch 12/20\n",
      "53/53 [==============================] - 26s 485ms/step - loss: 0.3841 - acc: 0.8589 - val_loss: 0.2985 - val_acc: 0.8789\n",
      "Epoch 13/20\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.3708 - acc: 0.8650 - val_loss: 0.3008 - val_acc: 0.8820\n",
      "Epoch 14/20\n",
      "53/53 [==============================] - 25s 480ms/step - loss: 0.3662 - acc: 0.8667 - val_loss: 0.3407 - val_acc: 0.8656\n",
      "Epoch 15/20\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8686\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00013499999768100678.\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.3605 - acc: 0.8681 - val_loss: 0.3010 - val_acc: 0.8805\n",
      "Epoch 16/20\n",
      "53/53 [==============================] - 26s 482ms/step - loss: 0.3510 - acc: 0.8716 - val_loss: 0.2878 - val_acc: 0.8906\n",
      "Epoch 17/20\n",
      "53/53 [==============================] - 25s 479ms/step - loss: 0.3432 - acc: 0.8728 - val_loss: 0.3097 - val_acc: 0.8805\n",
      "Epoch 18/20\n",
      "53/53 [==============================] - 26s 481ms/step - loss: 0.3466 - acc: 0.8734 - val_loss: 0.2674 - val_acc: 0.8984\n",
      "Epoch 19/20\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.3385 - acc: 0.8745 - val_loss: 0.3106 - val_acc: 0.8867\n",
      "Epoch 20/20\n",
      "53/53 [==============================] - 26s 494ms/step - loss: 0.3399 - acc: 0.8754 - val_loss: 0.2977 - val_acc: 0.8875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainData = pd.read_csv(\"../input/fashionmnist/fashion-mnist_train.csv\")\n",
    "trainLabel = trainData.iloc[:,0]\n",
    "trainLabel = trainLabel.to_numpy()\n",
    "trainOneHot = tf.keras.utils.to_categorical(trainLabel,10)\n",
    "\n",
    "trainImages = trainData.iloc[:,1:]\n",
    "trainImages = trainImages.to_numpy()\n",
    "trainImagesCNN = trainImages.reshape((60000,28,28,1))\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(trainImagesCNN, trainOneHot, test_size = 0.10, random_state=42) \n",
    "\n",
    "FashionTrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.,\n",
    "                                   brightness_range=[0.4,1.0],\n",
    "                                   rotation_range=30,\n",
    "                                   width_shift_range=0.20,\n",
    "                                   height_shift_range=0.20,\n",
    "                                   shear_range=10,\n",
    "                                   zoom_range=0.20,\n",
    "                                   horizontal_flip=False)\n",
    "\n",
    "FashionValid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n",
    "                              patience=3, verbose = 1)\n",
    "\n",
    "history = model1.fit_generator(FashionTrain_datagen.flow(x_train, y_train, batch_size= 1024),\n",
    "                              epochs=20,callbacks=[reduce_lr],validation_steps = 40,shuffle = True,\n",
    "                              validation_data=FashionValid_datagen.flow(x_valid, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,773,642\n",
      "Trainable params: 1,771,082\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\n",
    "trainLabel = trainData.iloc[:,0]\n",
    "trainLabel = trainLabel.to_numpy()\n",
    "trainOneHot = tf.keras.utils.to_categorical(trainLabel,10)\n",
    "\n",
    "trainImages = trainData.iloc[:,1:]\n",
    "trainImages = trainImages.to_numpy()\n",
    "trainImagesCNN = trainImages.reshape((60000,28,28,1))\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(trainImagesCNN, trainOneHot, test_size = 0.10, random_state=42) \n",
    "\n",
    "validationData = pd.read_csv('/kaggle/input/Kannada-MNIST/Dig-MNIST.csv')\n",
    "validationImages = validationData.iloc[:,1:]\n",
    "validationImages = validationImages.to_numpy()\n",
    "validationImagesCNN = validationImages.reshape((10240,28,28,1))\n",
    "\n",
    "validationLabel = validationData.iloc[:,0]\n",
    "validationLabel = validationLabel.to_numpy()\n",
    "validationOneHot = tf.keras.utils.to_categorical(validationLabel,10)\n",
    "\n",
    "KannadaTrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.,\n",
    "                                   brightness_range=[0.4,1.1],\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.20,\n",
    "                                   height_shift_range=0.20,\n",
    "                                   shear_range=10,\n",
    "                                   zoom_range=0.20,\n",
    "                                   horizontal_flip=False)\n",
    "\n",
    "KannadaValid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "model2 = tf.keras.Sequential()\n",
    "\n",
    "for layer in model1.layers[:-1]: # this is where I changed your code\n",
    "    model2.add(layer)    \n",
    "\n",
    "# Freeze the layers \n",
    "#for layer in model2.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "# Add 'softmax' instead of earlier 'prediction' layer.\n",
    "model2.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0015), loss = \"categorical_crossentropy\" , metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 53 steps, validate for 40 steps\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 27s 507ms/step - loss: 0.4322 - acc: 0.8711 - val_loss: 0.0325 - val_acc: 0.9922\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 25s 480ms/step - loss: 0.0860 - acc: 0.9747 - val_loss: 0.0152 - val_acc: 0.9945\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 26s 485ms/step - loss: 0.0668 - acc: 0.9799 - val_loss: 0.0224 - val_acc: 0.9945\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 26s 482ms/step - loss: 0.0588 - acc: 0.9819 - val_loss: 0.0283 - val_acc: 0.9922\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 26s 484ms/step - loss: 0.0523 - acc: 0.9840 - val_loss: 0.0108 - val_acc: 0.9961\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.0487 - acc: 0.9855 - val_loss: 0.0125 - val_acc: 0.9969\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 26s 482ms/step - loss: 0.0452 - acc: 0.9864 - val_loss: 0.0122 - val_acc: 0.9945\n",
      "Epoch 8/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9865\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00045000000391155477.\n",
      "53/53 [==============================] - 26s 481ms/step - loss: 0.0435 - acc: 0.9865 - val_loss: 0.0196 - val_acc: 0.9945\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 26s 484ms/step - loss: 0.0343 - acc: 0.9895 - val_loss: 0.0054 - val_acc: 0.9984\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 26s 482ms/step - loss: 0.0310 - acc: 0.9906 - val_loss: 0.0072 - val_acc: 0.9984\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 26s 488ms/step - loss: 0.0315 - acc: 0.9904 - val_loss: 0.0074 - val_acc: 0.9969\n",
      "Epoch 12/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9914\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00013499999768100678.\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.0284 - acc: 0.9914 - val_loss: 0.0079 - val_acc: 0.9969\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 25s 476ms/step - loss: 0.0276 - acc: 0.9914 - val_loss: 0.0073 - val_acc: 0.9984\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 25s 479ms/step - loss: 0.0268 - acc: 0.9915 - val_loss: 0.0046 - val_acc: 0.9984\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 26s 484ms/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.0049 - val_acc: 0.9984\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 25s 475ms/step - loss: 0.0248 - acc: 0.9924 - val_loss: 0.0064 - val_acc: 0.9984\n",
      "Epoch 17/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9923\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.049999843118712e-05.\n",
      "53/53 [==============================] - 25s 480ms/step - loss: 0.0243 - acc: 0.9923 - val_loss: 0.0060 - val_acc: 0.9984\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 25s 480ms/step - loss: 0.0247 - acc: 0.9924 - val_loss: 0.0056 - val_acc: 0.9984\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 25s 479ms/step - loss: 0.0243 - acc: 0.9925 - val_loss: 0.0055 - val_acc: 0.9984\n",
      "Epoch 20/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9933\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.2149999747634865e-05.\n",
      "53/53 [==============================] - 25s 477ms/step - loss: 0.0222 - acc: 0.9933 - val_loss: 0.0057 - val_acc: 0.9984\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 25s 481ms/step - loss: 0.0243 - acc: 0.9925 - val_loss: 0.0062 - val_acc: 0.9984\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 25s 481ms/step - loss: 0.0249 - acc: 0.9925 - val_loss: 0.0056 - val_acc: 0.9984\n",
      "Epoch 23/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9926\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.6449999242904594e-06.\n",
      "53/53 [==============================] - 26s 494ms/step - loss: 0.0246 - acc: 0.9927 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 26s 486ms/step - loss: 0.0242 - acc: 0.9921 - val_loss: 0.0052 - val_acc: 0.9984\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 25s 481ms/step - loss: 0.0239 - acc: 0.9927 - val_loss: 0.0052 - val_acc: 0.9984\n",
      "Epoch 26/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9928\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0934999636447174e-06.\n",
      "53/53 [==============================] - 25s 481ms/step - loss: 0.0244 - acc: 0.9928 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 26s 486ms/step - loss: 0.0234 - acc: 0.9928 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 26s 484ms/step - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 29/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9926\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.280499754509947e-07.\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 26s 488ms/step - loss: 0.0226 - acc: 0.9931 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 26s 486ms/step - loss: 0.0246 - acc: 0.9924 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 32/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9931\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.841498922469327e-08.\n",
      "53/53 [==============================] - 26s 483ms/step - loss: 0.0240 - acc: 0.9930 - val_loss: 0.0054 - val_acc: 0.9984\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 25s 480ms/step - loss: 0.0232 - acc: 0.9929 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 26s 486ms/step - loss: 0.0224 - acc: 0.9931 - val_loss: 0.0052 - val_acc: 0.9984\n",
      "Epoch 35/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9928\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.9524495914756698e-08.\n",
      "53/53 [==============================] - 26s 493ms/step - loss: 0.0243 - acc: 0.9927 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 26s 482ms/step - loss: 0.0221 - acc: 0.9934 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 26s 481ms/step - loss: 0.0230 - acc: 0.9926 - val_loss: 0.0054 - val_acc: 0.9984\n",
      "Epoch 38/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9927\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 8.85734898758983e-09.\n",
      "53/53 [==============================] - 25s 476ms/step - loss: 0.0234 - acc: 0.9927 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 25s 477ms/step - loss: 0.0238 - acc: 0.9922 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 25s 479ms/step - loss: 0.0239 - acc: 0.9929 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 41/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9929\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.6572045896955387e-09.\n",
      "53/53 [==============================] - 25s 472ms/step - loss: 0.0233 - acc: 0.9929 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 25s 477ms/step - loss: 0.0240 - acc: 0.9926 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 25s 476ms/step - loss: 0.0230 - acc: 0.9928 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 44/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9925\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 7.971614035540142e-10.\n",
      "53/53 [==============================] - 25s 468ms/step - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 25s 470ms/step - loss: 0.0239 - acc: 0.9928 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 25s 475ms/step - loss: 0.0227 - acc: 0.9927 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 47/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9924\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.3914841773553516e-10.\n",
      "53/53 [==============================] - 26s 484ms/step - loss: 0.0246 - acc: 0.9925 - val_loss: 0.0054 - val_acc: 0.9984\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 25s 476ms/step - loss: 0.0232 - acc: 0.9928 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 25s 474ms/step - loss: 0.0233 - acc: 0.9931 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 50/50\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9926\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 7.17445269859951e-11.\n",
      "53/53 [==============================] - 25s 475ms/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0053 - val_acc: 0.9984\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit_generator(KannadaTrain_datagen.flow(x_train, y_train, batch_size= 1024),\n",
    "                              epochs=50,callbacks=[reduce_lr],validation_steps = 40,shuffle = True,\n",
    "                              validation_data=KannadaValid_datagen.flow(x_valid, y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv('/kaggle/input/Kannada-MNIST/test.csv')\n",
    "testImages = testData.iloc[:,1:]\n",
    "testImages = testImages.to_numpy()\n",
    "testID = testData.iloc[:,0]\n",
    "testID = testID.to_numpy()\n",
    "\n",
    "numImages,_ = testImages.shape\n",
    "testImagesCNN = testImages.reshape((numImages,28,28,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict_classes(testImagesCNN/255.)\n",
    "submission = pd.read_csv('../input/Kannada-MNIST/sample_submission.csv')\n",
    "submission['label'] = predictions\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
